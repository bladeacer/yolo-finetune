\documentclass[12pt,a4paper]{article}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=blue,
    urlcolor=blue
}
\setlength{\parindent}{0pt}

\begin{document}
  \begin{titlepage}
     \vspace*{\stretch{1.0}}
     \begin{center}
        \Large\textbf{yolo-fine: Finetuning YOLOv11 on buffalo and camel labels}\\
        \large\textrm{Nicholas Wen, Nanyang Polytechnic}\\
        \large\textrm{13 December 2025}
     \end{center}
     \vspace*{\stretch{2.0}}
  \end{titlepage}

  \section{Introduction}
  \subsection{Foreword}
  This was made for my assignment submission for the Advanced Topics in AI module. For this assignment, I was tasked with finetuning a pre-trained Object Detection model to detect 2 classes of objects of my choice. I chose to finetune YOLOv11 on the `camel' and `buffalo' object classes. Approximately 1,500 images were labelled across these two object classes after the data preparation process, the exact image count as of 11 December 2025 is \textbf{1,643 images} after the data labelling process. The labelling process took about 2 to 3 weeks.

  \subsection{Links}
  \href{https://github.com/bladeacer/yolo-finetune}{GitHub source}, \href{https://huggingface.co/spaces/IT3103-2025S2/232343X}{HuggingFace Space}, \href{https://universe.roboflow.com/stuff-avvl2/yolo-fine-y444l}{Labelled dataset} on Roboflow.

  \href{https://images.cv/dataset/water-buffalo-image-classification-dataset}{Buffalo dataset} and \href{https://images.cv/dataset/camel-image-classification-dataset}{Camel dataset}.

  This link contains \href{https://github.com/amikelive/coco-labels/blob/master/coco-labels-2014_2017.txt}{COCO 2014 - 2017 labels}.

    \noindent\rule{\textwidth}{0.4pt}
    \section{Data preparation}
    During the data labelling process, I encountered some data which would be not beneficial to include in the dataset. For example, Camel dataset having cartoon, painting illustrations or Buffalo dataset having buffaloes that are too small for meaningful labelling.

    In other words, I did not include low resolution, irrelevant images and images in which the subject of interest is too far away or heavily overlapping. These were discarded so as to prevent irrelevant data from affecting training accuracy.

    Images with slight overlap with boundaries which were difficult to draw bounding boxes for were segmented and labelled properly with the help of \href{https://ai.meta.com/sam3/}{SAM3}. All data labelling was done using \href{https://roboflow.com/}{Roboflow}. Both datasets were large collections of unlabelled images.

    \subsection{Data augmentation}
    Data augmentation was employed to complement data discovery, since the image dataset comprises camels and buffaloes images taken from varying distances, angles or conditions. For example, buffaloes on land and in water or mud; whether the camel or buffalo is being ridden on.

    Data augmentation was primarily done to reduce the likelihood of the model overfitting, in that it tries to memorise the training dataseset instead of generalising patterns properly. Data augmentation steps such as Horizontal Flip, Random Crop, Random Rotation and Random Brightness was applied.

    \noindent\rule{\textwidth}{0.4pt}
    \section{Model training}
    A 80-10-10 train-test-validation split was employed on the dataset for evaluation purposes. The model training was runned locally on my laptop.

    As my hardware specifications are not the best, I had to run model training outside of the Jupyter Lab instance to prevent Out of Memory issues from CUDA running within the Jupyter kernel. Speaking of CUDA, my discrete GPU (Nvidia RTX 2050) does not support CUDA. However, model training still runs much faster on the discrete GPU than on the CPU.

    For images with large overlaps or where it would be not meaningfulto draw sufficiently large bounding boxes, SAM3 was employed for proper segmentation. These segmented images were more accurate to the data itself but ended up not being very meaningful for training.

    \subsection{Segmentation versus detection}
    At runtime, Ultralytics itself handles the mixed segment-detect dataset robustly, being able to discard the segment parts of the dataset which were not relevant to our analysis. The segments were left in as some images had both segmentation and bounding boxes applied to them.

    Even after discarding, we have about \textbf{8,000+} bounding box instances, which approximately is \textbf{1,500+} before augmentation.The image count to bounding box ratio is fine here, as there are plenty of images where we were able to draw multiple bounding boxes which compensated for the images for which we could not draw bounding boxes for. We will demonstrate a brief proof to show where these numbers are derived.

    \begin{align*}
        &\text{Let } x \text{ be the number of images in the original dataset before validation} \\
        &\text{split, } y \text{ be the number of data augmentations used and } z \text{ be the ratio}\\
        &\text{between each image and a bounding box instance}\\
        &\text{It is established that } x = 1,643, y = 4 \text { and } z \geq 1 \\
        &\therefore x \times (4+1) = 8,215 \text{ Total images after augmentation} \\
        & \Rightarrow  \text { Total bounding box instances after augmentation } \geq 8,215 \text { (Q.E.D.)}
    \end{align*}

    Note: 
    We add 1 to the multiplication to account for the base image set.
    $z \geq 1$ holds true as on average I drew one or more bounding boxes per image.

    \subsection{Evaluation}
    We make use of \textit{wandb} to evaluate metrics of our trained model, as it is more accurate than running benchmarks locally. After about six iterations of different versions of the model with hyperparameter modifications, these are the best results we have achieved:

    The best \textbf{mAP50} was \textbf{0.84335}, indicating that the model is able to predict accurately 84\% of the time when the predicted bounding box overlaps the ground truth object by at least 50\%. This is often considered the metric for detection presence (whether the object was found).

    The best \textbf{mAP50-95} was \textbf{0.65193}, indicating that the model is able to place bounding boxes with good accuracy for strict Intersection over Union (IoU) thresholds. This metric averages performance across IoU thresholds from 50\% to 95\% and is the industry standard for localization precision.

    The best \textbf{Precision} was at \textbf{0.83696}, indicating that \textbf{83.7\% of the objects the model predicted were actually correct detections} (True Positives), indicating a low rate of False Positive detections.

    The best \textbf{Recall} was at \textbf{0.80574}, indicating that \textbf{80.6\% of all actual objects in the dataset were successfully found by the model}, indicating a low rate of False Negative detections.

    These metrics are decent for our classes which have more complex features than simpler objects in the COCO dataset like cup and bowl. \textit{wandb} metrics in greater detail can be found in the \href{https://github.com/bladeacer/yolo-finetune/blob/main/analysis.ipynb}{Jupyter Notebook} and \href{https://github.com/bladeacer/yolo-finetune/blob/main/images/}{here}.

    \subsection{Finetuning}
    During the training process, certain parameters had to be finetuned to reduce bottlenecks and improve performance metrics. The epoch count was increased from the default 30 to 35, with a patience metric of 5 so that the model does not keep training without any meaningful improvement. Other granular changes include reducing initial and final learning rate, or switching the optimiser to \textit{AdamW}.

    These changes were performed so as to reduce the likelihood of the model from overfitting. The \textit{deterministic} flag was also disabled for better robustness and less overhead. Most parameter changes aimed to reduce learning rate, resulting in a more stable training process (in terms of metrics like validation loss).

    Benchmarks on various hyperparameter settings can be \href{https://github.com/bladeacer/yolo-finetune/blob/main/benchmarks.md}{found here}.
    
    \subsection{Deployment}
    After we have successfully created and finetuned the model, we export it as an \href{https://huggingface.co/bladeacer/yolo-fine/tree/main}{OpenVino model artefact}. Inference and streaming results can be \href{https://github.com/bladeacer/yolo-finetune/tree/main/results}{found here}. We then deployed the model itself to a HuggingFace Spaces instance.

    \noindent\rule{\textwidth}{0.4pt}
    \section{Conclusion}
    In conclusion, we have prepared, trained and deployed a finetuned instance of YOLOv11.

    There are inherent limitations in the existing dataset, such as a lack of images for younger camels or buffaloes. I have tried to keep human error to a minimum when labelling dataset, although I cannot fully guarantee 100\% accuracy in the labelling process. There is also less data in the dataset for camels as compared to buffaloes, leading to lower precision and recall for camels.

    Since the object classes are not in the COCO dataset, they are unfamiliar to YOLO. This results in a slightly lower accuracy as the model was not trained specifically on more common classes like balloon.

    \noindent\rule{\textwidth}{0.4pt}
    \section{Credits}
    \LaTeX \space was used to create this report.

    Ms Liang Nanying for guidance and support during the creation of this report.

    Ultralytics documentation for detailing parameters. This document is hosted on GitHub. Model training was done on Roboflow.

\end{document}
